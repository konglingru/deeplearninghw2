DPMesh: Exploiting Diffusion Prior for Occluded Human Mesh Recovery
Yixuan Zhu1*
Ao Li2*
Yansong Tang2†
Wenliang Zhao1
Jie Zhou1
Jiwen Lu1
1Department of Automation, Tsinghua University
2Tsinghua Shenzhen International Graduate School, Tsinghua University
Abstract
The recovery of occluded human meshes presents chal-
lenges for current methods due to the difﬁculty in extract-
ing effective image features under severe occlusion. In this
paper, we introduce DPMesh, an innovative framework for
occluded human mesh recovery that capitalizes on the pro-
found diffusion prior about object structure and spatial re-
lationships embedded in a pre-trained text-to-image diffu-
sion model. Unlike previous methods reliant on conven-
tional backbones for vanilla feature extraction, DPMesh
seamlessly integrates the pre-trained denoising U-Net with
potent knowledge as its image backbone and performs a
single-step inference to provide occlusion-aware informa-
tion.
To enhance the perception capability for occluded
poses, DPMesh incorporates well-designed guidance via
condition injection, which produces effective controls from
2D observations for the denoising U-Net. Furthermore, we
explore a dedicated noisy key-point reasoning approach to
mitigate disturbances arising from occlusion and crowded
scenarios.
This strategy fully unleashes the perceptual
capability of the diffusion prior, thereby enhancing accu-
racy. Extensive experiments afﬁrm the efﬁcacy of our frame-
work, as we outperform state-of-the-art methods on both
occlusion-speciﬁc and standard datasets. The persuasive
results underscore its ability to achieve precise and robust
3D human mesh recovery, particularly in challenging sce-
narios involving occlusion and crowded scenes. Code is
available at https://github.com/EternalEvan/
DPMesh.
1. Introduction
The goal of human mesh recovery involves estimating the
3D human pose and shape from either monocular or multi-
view images and videos. Over the past decade, this ﬁeld has
evolved into a burgeoning and captivating research problem,
gaining prominence for its extensive applications in ﬁlm-
*Equal contribution.
†Corresponding authors.
Pose Skeletons
Depth Map
Spatial Interaction
Human Structure
Control
Net
denoising
step-by-step
Conditioned Image Generation
Control
Net
Denoising U-Net
one-step
extraction 
Input Image
Noisy Heatmap
Predicted Meshes
DPMesh
Denoising U-Net
“wear helmet,
play football”
“astronauts,
play soccer”
Figure 1. Main idea of the proposed DPMesh framework. We
design an innovative framework to fully exploit rich prior knowl-
edge about human structure and spatial interaction of the pre-
trained diffusion model for challenging occluded human mesh re-
covery task. By simply adapting the denoising U-Net as a single-
step backbone with spatial conditions, we achieve accurate human
mesh recovery even under severe occlusions.
making, game development, and sports. In recent years, a
plethora of approaches [5, 8, 21, 23–25, 30, 39, 47, 49, 55]
grounded in deep learning have emerged, paving the way
to address this inherently ill-posed problem by effectively
regressing body parameters from image features.
Nonetheless, extracting more effective information from
monocular images in complex scenarios (e.g., occlusions
and crowded environments), remains a pivotal challenge.
Existing methods address the intricacies of mesh recovery
in complex scenarios by incorporating 2D prior knowledge
as hints, drawing the models’ attention to visible body parts
and reinforcing their 2D alignment proﬁciency.
Follow-
ing this line, mainstream methods [7, 28] intuitively ap-
ply off-the-shelf key-point detectors to achieve coarse hu-
man joints, others such as [24] and [57] introduce partial
segmentation masks and UV maps as pixel-level knowl-
This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.
Except for this watermark, it is identical to the accepted version;
the final published version of the proceedings is available on IEEE Xplore.
1101
(a) Conventional methods
࣡
ȣ
ߚ
ߨ
࣢
ݔ
ȣ଴
ȣ்
濁濁濁
Diffusion
Denoising
(b) Diffusion-based methods
U-Net
࣡
濁濁濁
濁濁濁
濁濁濁
ݔ
(c) DPMesh
Conditions
࣢
ȣ
ߚ
ߨ
U-Net
ݖ଴
࣡
One-step
One-step
Multi-steps
Figure 2. Comparison of current methods and the proposed DPMesh. (a) Conventional methods [21, 25] apply a feature extractor G and
a regressor H to obtain SMPL parameters. (b) Diffusion-based methods [4, 10, 12] propose an iterative framework that harnesses multiple
denoising steps to progressively reﬁne the pose parameters from random noise. (c) Distinct from previous diffusion-based techniques, our
DPMesh employs the pre-trained denoising U-Net as the backbone G, executing a one-step inference to furnish informative features for the
regressor. This novel framework transfers the potent perception knowledge in generative models onto conventional frameworks.
edge. Despite these efforts, persistent shortcomings become
apparent, particularly when confronted with severe occlu-
sion, since they excessively depend on 2D alignment and
disregard the vivid information embedded in natural im-
ages. Consequently, disturbances to the 2D detector due
to noise and occlusion signiﬁcantly impact accuracy, yield-
ing unsatisfactory outcomes. Recently, Diffusion Models
(DMs) [16, 42] have introduced a step-by-step generation
framework, showcasing remarkable image synthesis capac-
ity and producing visually appealing results. Inspired by
DMs, recent works like [4, 10, 12] utilize the generative
approach proposed by diffusion models for pose estima-
tion, achieving high-accuracy results. However, diffusion-
based methods suffer from repeated iterations and neglect
the learned knowledge for image processing within text-to-
image diffusion models, causing the potential of diffusion
not fully exploited. More recent studies [51, 56, 58] inves-
tigate the pre-trained diffusion model for 3D-related tasks,
e.g., image synthesis from the depth map, text-to-3D gener-
ation and depth estimation. It has been veriﬁed that the pre-
trained diffusion model can provide structure-aware knowl-
edge for 3D generation and perception tasks. Although dif-
fusion models possess rich knowledge of 3D structure and
spatial interaction from generative training, the challenge
persists in effectively leveraging these capabilities for com-
plex regression tasks like occluded human mesh recovery.
To overcome the aforementioned challenges, we present
DPMesh, a simple yet effective framework for occluded hu-
man mesh recovery. DPMesh employs a pre-trained text-to-
image diffusion model as the backbone, fully leveraging its
potent knowledge of the 3D structure and spatial relation-
ships learned from generative training, hence yielding a ro-
bust estimator for occluded poses, as illustrated in Figure 1.
Our primary goal is to harness both the high-level and low-
level visual concepts within a pre-trained diffusion model
for the demanding occluded pose estimation task. Instead of
following the time-consuming step-by-step denoising pro-
cess, we replace conventional image backbones with the
pre-trained denoising U-Net and perform an efﬁcient single-
step inference style with designed conditions as guidance,
as depicted in Figure 2. Considering the pre-training of
the diffusion model on text-to-image generation tasks, we
confront two challenges: (1) preserving the learned knowl-
edge within the pre-trained diffusion model and adapting
it to the occluded human mesh recovery task, and (2) de-
signing appropriate conditions and controls to enhance the
model’s perception ability. To address these issues, we in-
troduce an efﬁcient framework to tailor the diffusion model
for mesh recovery, leveraging an effective condition injec-
tion. To align with the original diffusion model and facil-
itate the interaction between image features and 2D prior
information, we reﬁne the spatial information from an off-
the-shelf detector and inject the diffusion model with these
conditions. This yields detailed knowledge of the 2D po-
sition and the key-points uncertainty. The processed 2D
information serves as guidance for the diffusion model, ul-
timately producing rich visual content, encompassing both
human structure and spatial interaction for the subsequent
regressor. Furthermore, we present a noisy key-point rea-
soning approach to improve the robustness of our model,
rendering it more stable for occlusion and crowds.
We conduct extensive experiments on various occlu-
sion benchmarks 3DPW-OC [50, 57], 3DPW-PC [47, 50],
3DOH [57], 3DPW-Crowd [7, 50] and CMU-Panoptic [20],
as well as the standard benchmark 3DPW test split [50]. Re-
markably, without any ﬁnetuning on the 3DPW training set,
our DPMesh achieves an exciting performance, surpassing
previous state-of-the-art methods and demonstrating signif-
icantly improved accuracy. Speciﬁcally, we achieve MPJPE
values of 70.9, 82.2, 79.9, and 73.6 on 3DPW-OC, 3DPW-
PC, 3DPW-Crowd, and 3DPW test split, respectively, un-
derscoring the proﬁciency of our framework. Furthermore,
we carry out comprehensive ablation studies to highlight the
effectiveness of the diffusion-based backbone, the condition
construction and the designed noisy key-point reasoning.
1102
2. Related Work
Human Mesh Recovery. During the past decade, param-
eterized human model [1, 33, 40] has been widely used
to express 3D human pose and shape. Many proceeding
works explore approaches to estimate accurate model pa-
rameters from monocular images [5, 8, 13, 14, 21, 23–
25, 30, 39, 49, 54, 55]. They usually regress parameters
from extracted image features.
Some [34, 55] leverage
2D and 3D visual observations to enhance the 2D align-
ment of image features. Nevertheless, they always fall short
when confronted with complex scenarios, e.g. occlusion
and crowded environments since the conventional back-
bones and estimators provide vague information about the
occluded region for the regressor. To handle this challenge,
a series of methods [6, 22, 24, 28, 48, 57] propose effec-
tive approaches involving segmentation masks, center maps
and 3D representations to improve the 2D and 3D align-
ments. However, they have limitations since they pay too
much attention to enhancing the usage of the 2D and 3D ob-
servations and ignore the quality of image features, which
are fundamental and signiﬁcant. Most recently, methods
based on diffusion models [4, 10, 12] introduce an iterative
framework to estimate human poses in the repeated denois-
ing process. Though they achieve satisfying accuracy, they
suffer from extensive time-consuming and do not fully ex-
ploit the rich knowledge in diffusion models.
Diffusion Models. Diffusion denoising probabilistic mod-
els, commonly referred to as diffusion models [16, 42], have
emerged as a prominent family of generative models, show-
casing remarkable synthesis quality and controllability. The
core concept of the diffusion model involves training a de-
noising autoencoder to estimate the inverse of a Marko-
vian diffusion process [45]. Through generative training
on large-scale datasets with image-text pairs (e.g., LAION-
5B [44]), diffusion models acquire a powerful capability to
generate high-quality images with diverse content and rea-
sonable structures. This proﬁciency is harnessed during dif-
fusion sampling, which can be perceived as a progressive
denoising procedure that necessitates repeated inference of
the denoising autoencoder. Recently, [56] propose a con-
trollable architecture, named ControlNet, to add spatial con-
trols, e.g., depth maps and human poses, to pre-trained dif-
fusion models, broadening their applications to controlled
image generation. Although originally tailored to 2D text-
to-image tasks, pre-trained diffusion models also possess
rich knowledge about object structure and spatial interac-
tion. They can adapt to various 3D-related tasks like image
synthesis from depth map, text-to-3D generation and depth
estimation, as explored in [51, 56, 58]. However, fully ex-
ploiting the structure-aware generative prior in the diffusion
model for complex mesh recovery, especially in occluded
and crowded scenarios, still poses a signiﬁcant challenge
due to the need for proﬁcient visual perception capability.
3. Methods
In this section, we present DPMesh, an effective framework
for occluded human mesh recovery with the pre-trained dif-
fusion prior and proper conditional control. We will start by
reviewing the background of diffusion models with condi-
tional control and the human body model. Then we will pro-
vide a detailed walkthrough of the entire pipeline to intro-
duce our designs in DPMesh. This includes how we lever-
age the generative diffusion prior for the human recovery
task and inject valuable conditions to guide the denoising
U-Net. Moreover, we will present a noisy key-point reason-
ing approach to enhance the robustness of our model. The
overall framework of our DPMesh is illustrated in Figure 3.
3.1. Preliminaries
Conditional Control for Diffusion Models.
Diffusion
models achieve high controllability thanks to the effective
cross-attention layers in the denoising U-Net ϵθ [43] which
bridges a way for the interactions between image features
and various conditions. Recently, ControlNet [56] success-
fully enhances the ﬁne-grained spatial control on latent dif-
fusion model (LDM) [42] by leveraging a trainable copy
of the encoding layers in the denoising U-Net as a strong
backbone for learning diverse conditional controls. During
the training of the ControlNet framework, images are ﬁrst
projected to latent representations z0 by a trained VQGAN
consisting of the encoder E and the decoder D. Denoting zt
as the noisy image at t-th timestep, it is produced by:
zt = √¯αtz0 +
√
1 −¯αtϵ,
(1)
where ¯αt = t
s=1 αs and ϵ ∼N(0, I). Given the noisy
image and conditions, the training objective of the Control-
Net framework can be derived as:
LCLDM = Ez0,t,ct,cf,ϵ

∥ϵ −ϵθ(zt, t, ct, cf)∥2
2

,
(2)
where zt is computed from Equation (1), ct denotes the text
condition embedding extracted from frozen CLIP [41] text
encoder and cf is a task-speciﬁc condition, such as human
skeleton poses or canny maps. To prevent harmful noise
that inﬂuences the hidden states of neural network layers
at the start of training, ControlNet applies zero convolution
layers for the trainable copy branch. The condition branch
consumes cf as input and injects the outcomes to the out-
put blocks of diffusion model ϵθ. In order to keep gener-
ation capability and reduce computational costs, it freezes
the parameters in ϵθ. By utilizing the ﬁne-grained condi-
tions, ControlNet successfully achieves controllable human
image generation with various conditions like 2D skeletons.
Human Body Model. We use SMPL [33] model to pa-
rameterize human body mesh. SMPL represents 3D human
body with three vectors, denoted by pose Θ ∈R72, shape
1103
resize
C
Denoising U-Net ߳ఏ
Control
Net
࣐
ܿ୨
ܿ୲
cross-attention 
maps
multi-scale
feature maps
ݖ଴
input image ݔ
࣠ௌ
Concat
Add
Locked
L
visible joints ܬଶୈ
(a) Mesh Recovery with Conditioned Diffusion Prior
ए
output mesh ࣧ
ऎ
Regressor
ߚ, ߨ
ȣ
noisy heatmap ܪଶୈ
ܮ୒୏ோ
Noisy
heatmap
ݖ଴
Student
Model
࣠ௌ
்࣠
GT
heatmap
ݖ଴
Teacher
Model
h
(b) Noisy Key-point Reasoning
C
C
Conv
Conv
Figure 3. The overall framework of DPMesh. Given the input image x, pre-detected 2D key-points J2D, and generated heatmap H2D,
our framework begins with the extraction of image features FS through a single denoising step using the pre-trained diffusion model.
This process is guided by the designed spatial conditions. Then, we input F S to the regressor to predict SMPL parameters Θ, β and π,
ultimately generating the ﬁnal mesh. To further enhance the estimation robustness against noisy 2D observations, we leverage a noisy
key-point reasoning approach. This involves pre-training a teacher model with ground truth heatmaps and then training a student model
using noisy heatmaps by aligning the feature maps computed from the locked teacher and the student.
β ∈R10 and camera parameters π ∈R3. The body mesh is
generated by a differentiable function M(Θ, β) ∈R6890.
Then we can obtain the 3D joint coordinates by J3D =
WM ∈RN×3, where W is a pre-trained linear regressor
and N represents the number of joints. With the predicted
camera parameters π, we can obtain reprojected 2D joints
J2D = Π(J3D, π) ∈RN×2 by perspective projection.
3.2. Diffusion Prior for Occluded Mesh Recovery
Overview. Our primary goal is to fully exploit the pre-
trained diffusion model’s potential for occluded human
mesh recovery, leveraging its learned knowledge of the ob-
ject structure and spatial interactions. In contrast to previ-
ous methods involving repeated diffusion sampling, our ba-
sic idea is to simply employ the pre-trained diffusion model
as the image backbone, performing a single inference to ex-
tract features from image x. To provide effective guidance,
we adopt the condition injection to play an essential role
in processing pre-detected 2D observations into the condi-
tions. Then we utilize the noisy key-point reasoning ap-
proach to improve the occlusion awareness of our model to
further enhance the robustness of the proposed framework.
In conclusion, we propose DPMesh, which takes the image
and corresponding noisy key-point observations as inputs
and estimates the SMPL parameters Θ, β and π, collectively
denoted as the output y. This process can be formulated as:
pφ(y|x) = pφ3(y|F)pφ2(F|x, C)pφ1(C|x),
(3)
where F denotes the extracted feature maps and C repre-
sents the conditions. We will describe the role of each term
in Equation (3) along with their detailed designs.
Condition Injection with 2D HeatMap. pφ1(C|x) aims to
construct effective conditions, which is signiﬁcant since it
provides spatial guidance for the denoising U-Net backbone
ϵθ. Therefore we design the condition injection to introduce
high-level and spatial information that guides the backbone
to focus on the region of interest. For each cropped im-
age, we utilize an off-the-shelf 2D key-point detector [2] to
obtain 2D joints J2D along with their corresponding con-
ﬁdence and generate the heatmaps H2D ∈RN×H0×W0 as
conditioning input using 2-dimensional Gaussian kernels,
where N represents the number of detected key-points. Af-
ter that, we concatenate the heatmap H2D with the input
image z0 to obtain cj ∈R(N+Cz)×H0×W0. It is noteworthy
that, in the original ControlNet architecture, the condition-
ing image is fused with z0 by element-wise adding after
passing through convolution layers. However, we observe
that this addition signiﬁcantly damages the ﬁnal condition
quality. The preparation for cj can be expressed as:
cj = Concat(z0, H2D), H2D = Gaussian(J2D).
(4)
Then we employ the ControlNet architecture to process
ﬁne-grained conditions from cj and inject them to the image
features in the denoising U-Net ϵθ. The output of the i-th
layer Fi in the decoding layers of ϵθ is derived as:
Fi = F(Fi−1; θi) + Conv(F(cj; θcond); θConv),
(5)
where Fi−1 is the output of the previous block and F(·; θ)
denotes a trained neural network. θi, θcond represent the
1104
parameters within the denoising U-Net and ControlNet, re-
spectively. θConv is the parameters of zero convolution lay-
ers with both weights and bias initialized to zeros. Note that
in the original ControlNet architecture, θcond is a trainable
copy of the encoding blocks in the denoising U-Net.
Besides the ControlNet that provides controls for the de-
noising U-Net ϵθ, we also consider using the cross-attention
prompt to pinpoint more accurate spatial information. In
original diffusion models, the prompt ct conditions typi-
cally consist of text embeddings from frozen CLIP. How-
ever, in our framework, we replace text with visible 2D joint
coordinates J2D ∈RN×2. We then apply a two-layer MLP
to elevate the dimension of 2D joint coordinates to match
the text token dimension Dt, which is set to 768 in the pre-
trained diffusion model. Thus we obtain the auxiliary spa-
tial condition ct ∈RN×Dt. We take ct as prompt guidance
and send it to all cross-attention blocks in the denoising U-
Net. The construction of ct can be formulated as:
ct = MLP(J2D).
(6)
To sum up, we construct the condition set C consisting of cj
and ct and inject them into ϵθ through different passways.
Feature Extraction with Diffusion Prior. pφ2(F|x, C) is
dedicated to the feature extraction from the input image.
In contrast to previous methods that employ convolution-
based and Transformer-based backbones, we leverage the
pre-trained diffusion model as our backbone, exploiting
the visual perception capability within the denoising U-Net
learned from the generative training. As veriﬁed in [58],
there exist enough visual priors about the object structure
and spatial interactions in a pre-trained denoising U-Net ϵθ.
By unlocking the potential of ϵθ, we can tune the generative
capability in the pre-trained diffusion model to address the
human mesh recovery task. Motivated by this perspective,
we design a straightforward feature extractor implemented
by the pre-trained denoising U-Net, which receives effec-
tive guidance C from the condition injection. To prepare the
input images, we convert the cropped image x ∈RH×W ×C
from pixel space to the latent space with the frozen encoder
E to obtain the latent representation z0 ∈RH0×W0×Cz.
Then we feed z0 into the pre-trained U-Net ϵθ and extract
the multi-scale feature maps Fi, where i ∈{1, 4, 7}, from
the decoding layers as the implicit image features. Further-
more, we empirically observe that the cross-attention maps
Ti ∈R|ct|×Hi×Wi can provide signiﬁcant occlusion-aware
information indicating the invisible parts and explicit ob-
ject structure knowledge about the human pose and shape.
Therefore we concatenate the cross-attention maps with the
feature maps Fi and obtain the hierarchical feature maps
F ←{[Fi, Ti]}, which incorporate both implicit and ex-
plicit diffusion priors for subsequent regression.
SMPL Mesh Regressor. pφ3(y|F) represents the regressor
responsible for predicting the parameters of the body model
from feature maps F. To capture the body information in F,
we employ a cascade Transformer decoder for the regressor.
In order to provide sufﬁcient human pose priors and main-
tain the symmetry of the VQGAN framework, we train a
VQVAE on a large motion dataset [35] with massive SMPL
pose parameters to learn discrete representations for human
poses. During the ﬁnal regression, we predict the entry in-
dices of the learned codebook and feed the corresponding
pose embedding to the decoder D of the VQVAE to attain
the pose parameters Θ. As for shape and camera param-
eters (i.e. β and π), which are highly dependent on image
features, we directly regress them using linear layers.
3.3. Noisy Key-point Reasoning
As we introduce an off-the-shelf 2D key-point detector
for providing 2D observation hints, there naturally arises
a problem: How robust is our framework in the presence
of noisy key-points? The noisy key-points, often arising
from severe occlusion, can adversely impact the model’s
performance during evaluation. This consideration moti-
vates us to reinforce our backbone with extra supervision
to mitigate the model’s reliance on noisy key-points. To
achieve this, we leverage a self-supervised distillation ap-
proach, called Noisy Key-point Reason (NKR), that focuses
on 2D detection errors, including missing key-points, jitters
and mismatch. The core concept involves training a teacher
model adept at accurately encoding feature maps with pre-
cise ground truth key-points. Then we utilize the teacher’s
feature maps FT to guide and supervise the student’s fea-
ture maps FS. During the distillation process, we minimize
both the SimCLR loss [3] and MSE loss:
LNKR = LSimCLR + LMSE,
(7)
where the SimCLR loss is computed by:
LSimCLR = log
exp(Dist(F S
i , F T
i ))

i̸=j exp(Dist(F S
i , F T
j )).
(8)
The SimCLR loss extends the distance between the two
models’ features for different inputs while minimizing the
distance for the same input. The MSE loss also provides
additional supervision to align features between teacher and
student. This noisy key-point reasoning approach enhances
the robustness of our framework against 2D detection er-
rors, ensuring its stability under challenging occlusion.
3.4. Implementation
We use Stable Diffusion V1-5 [42] with ControlNet [56],
pre-trained for human-like image generation from 2D skele-
tons, as our image backbone. Following [7, 28], we take the
cropped image in 256 × 256 resolution as input and en-
code it into the latent code z0 ∈R4×32×32. We extract fea-
ture maps with the size 8, 16 and 32 and the cross-attention
1105
Table 1. Quantitative comparisons on occlusion benchmarks. The units for mean joint and vertex errors are in millimeters. Our DPMesh
demonstrates outstanding estimation accuracy across diverse occlusion conditions, underscoring the efﬁcacy of our framework.
Method
3DPW-OC
3DPW-PC
3DOH
3DPW-Crowd
MPJPE↓
PA-MPJPE↓
MPVE↓
MPJPE↓
PA-MPJPE↓
MPVE↓
MPJPE↓
PA-MPJPE↓
MPVE↓
MPJPE↓
PA-MPJPE↓
MPVE↓
SPIN [25]
95.5
60.7
121.4
122.1
77.5
159.8
110.5
71.6
124.2
121.2
69.9
144.1
PyMAF [55]
89.6
59.1
113.7
117.5
74.5
154.6
101.6
67.7
116.6
115.7
66.4
147.5
ROMP [47]
91.0
62.0
-
98.7
69.0
-
-
-
-
104.8
63.9
127.8
OCHMR [22]
112.2
75.2
145.9
-
-
-
-
-
-
-
-
-
PARE [24]
83.5
57.0
101.5
95.8
64.5
122.4
109.0
63.8
117.4
94.9
57.5
117.6
3DCrowdNet [7]
83.5
57.1
101.5
90.9
64.4
114.8
102.8
61.6
111.8
85.8
55.8
108.5
JOTR [28]
75.7
52.2
92.6
86.5
58.3
109.7
98.7
59.3
104.8
82.4
52.0
103.4
DPMesh (Ours)
70.9
48.0
88.0
82.2
56.6
105.4
97.1
59.0
106.4
79.9
51.1
101.5
maps in the same resolution from the denoising U-Net. To
maintain the learned knowledge in the pre-trained diffusion
model, we use LoRA [17] to unfreeze the linear layers in
cross-attention blocks, setting the rank of LoRA modules to
64. We ﬁnd that even ﬁne-tuning a small number of param-
eters via LoRA yields satisfying results. More details are
shown in the appendix ﬁle.
Finally, we obtain mesh vertices M(Θ, β) ∈R6890 and
3D joints J3D = WM ∈RN×3 with functions men-
tioned in 3.1. We reproject the body joints to the image
by J2D = Π(J3D, π) ∈RN×2. In the reprojection process,
we approximately estimate the focal length with the length
of the image diagonal following [29]. For the training ob-
jectives, we utilize wide-used losses on SMPL parameters,
2D joints and 3D joints when 3D joint annotations are avail-
able to optimize our framework. In conclusion, the entire
loss function can be formulated as:
L = λ2DL2D + λ3DL3D+
λSMPLLSMPL + λNKRLNKR.
(9)
The ﬁrst three terms are computed by:
L2D = ∥J2D −ˆ
J2D∥, L3D = ∥J3D −ˆ
J3D∥,
(10)
LSMPL = ∥Θ −ˆΘ∥+ ∥β −ˆβ∥,
(11)
where ˆ
J2D, ˆ
J3D, ˆΘ and ˆβ represent the ground truth anno-
tations of 2D joints, 3D joints, SMPL body pose parameters
and shape parameters, respectively.
4. Experiments
To verify the effectiveness of our proposed DPMesh, we
conduct comprehensive experiments and ablation studies
on the standard benchmark and various occlusion datasets.
We will introduce the experimental settings including the
implementation for training and evaluation. Subsequently,
we will present our main results and offer a comprehensive
analysis through detailed ablations.
4.1. Experiment Setup
Training Details. In alignment with previous works [7, 28],
we train our model on a hybrid dataset with 2D or 3D an-
noations, including Human3.6M [18], MuCo-3DHP [36],
MSCOCO [31], and CrowdPose [27]. We exclusively uti-
lize the training sets of these datasets, adhering to standard
split protocols. For the 2D dataset, we utilize their pseudo
ground-truth SMPL parameters [38]. During training, we
add realistic errors on the ground truth (GT) 2D pose fol-
lowing [5, 37] to simulate erroneous 2D pose, rather than
generating detected key-point results. We use AdamW opti-
mizer with a batch size of 16 and a weight decay of 1e-6.We
set the initial learning rate to 1e-4 and cut it to 1e-5 in the
last 5 epochs.
Evaluation Details. We evaluate our model on 3DPW [50]
test split, 3DOH [57] test split, 3DPW-PC [47, 50], 3DPW-
OC [50, 57], 3DPW-Crowd [7, 50] and CMU-Panoptic
dataset [20]. 3DPW-PC is the person-person occlusion sub-
set of 3DPW and 3DPW-OC is the person-object occlusion
subset of 3DPW. 3DOH is another person-object occlusion
dataset. The metrics we use are mean per joint position error
(MPJPE) in mm, Procrustes-aligned mean per joint position
error (PA-MPJPE) in mm for evaluating the accuracy of 3D
joints and mean per vertex error (MPVE) in mm for evaluat-
ing 3D mesh error. For the CMU-Panoptic dataset, we only
evaluate MPJPE following previous work [11, 21, 52, 53].
4.2. Comparisons on Occlusion Benchmark
3DPW-OC [50, 57] is a person-object occlusion subset of
3DPW and contains 20243 persons. Table 1 shows that our
DPMesh outperforms all competitors with 70.9 mm MPJPE
and 48.0 mm PA-MPJPE, demonstrating its promising ca-
pability in effectively handling complex in-the-wild scenes.
3DOH [57] is a person-object occlusion-speciﬁc dataset
that encompasses 1290 images in the test split. All meth-
ods we report are not ﬁne-tuned on the training split for fair
comparison. We achieve the best results on 97.1 MPJPE
and 59.0 PA-MPJPE, as shown in Table 1. We further ex-
hibit the qualitative ﬁndings in Figure 5. Our DPMesh pro-
ﬁciently manages heavy occlusion situations.
3DPW-PC [47, 50] is a person-person occlusion subset of
3DPW, comprising 2218 individuals. Images within this
dataset contain annotations for multiple persons, potentially
distracting the feature extractor.
As shown in Table 1,
DPMesh exhibits superior performance compared to previ-
ous methods with 82.2 MPJPE and 56.6 PA-MPJPE.
1106
3DCrowdNet
JOTR
DPMesh
3DCrowdNet
JOTR
DPMesh
Input image
Input image
Figure 4. Qualitative comparisons on 3DPW dataset [50]. Our DPMesh recovers accurate human meshes under challenging occlusions
and demonstrates an adept understanding of 3D body structures and spatial relationships. Notably, our method also excels in generating
plausible details for the obscured body parts, e.g., hands and legs, proving the robustness of DPMesh in handling complex scenarios.
Table 2. Quantitative comparisons on 3DPW [50] test split.
DPMesh seamlessly generalizes to previously unseen distribu-
tions, yielding robust results on real-world RGB videos.
Method
MPJPE↓
PA-MPJPE↓
MPVE↓
HMR [21]
130.0
76.7
-
GraphCMR [26]
-
70.2
-
SPIN [25]
96.9
56.2
116.4
PyMaf [55]
92.8
58.9
110.1
OCHMR [22]
89.7
58.3
107.1
ROMP [47]
89.3
53.5
105.6
PARE [24]
82.9
52.3
99.7
3DCrowdNet [7]
81.7
51.2
98.3
JOTR [28]
76.4
48.7
92.6
DPMesh (Ours)
73.6
47.4
90.7
3DPW-Crowd [7, 50] is a person crowded subset of 3DPW
and contains 1923 persons. DPMesh exceeds state-of-the-
art methods across all metrics.
As demonstrated in Ta-
ble 1, we reach the best result on 79.9 MPJPE and 51.1
PA-MPJPE compared with previous methods.
CMU-Panoptic [20] dataset is a multi-person indoor
dataset collected with multi-view cameras. In order to en-
sure a fair comparison, we choose 4 scenes for evaluation,
following [7, 19]. Results are shown in Table 3, and we
outshine other competitors on all video clips.
4.3. Comparisons on Standard Benchmark
3DPW [50] is a widely-used benchmark for human mesh
recovery, featuring 60 videos and 3D annotations of 35,515
individuals in its test split. For a fair comparison with other
methods, we do not ﬁne-tune our model on the 3DPW train
split. As presented in Table 2, our method achieves state-
of-the-art performance on the test split. We also show the
qualitative results in Figure 4, which demonstrate that our
DPMesh is robust in complex, wild scenes.
3DCrowdNet
JOTR
DPMesh
Input image
Figure 5.
Qualitative results on 3DOH dataset [50].
Our
DPMesh obtains satisfying estimation for complex poses.
4.4. Analysis
Effective Backbone with Diffusion Prior. We engage in
a comparative study between our diffusion-based backbone
with convolution-based backbones such as ResNet50 [15]
and HRNet-W48 [46], as well as transformer-based back-
bones like ViT-L-16 [9] and Swin-V2-L [32]. Note that
our selection includes both supervised pre-trained mod-
els (e.g., ResNet50) and self-supervised pre-trained mod-
els (e.g., Swin-V2-L). To implement our comparison, we
concatenate pre-detected heatmaps with early-stage image
features and ﬁne-tune each backbone for 30 epochs. As
revealed in Table 4, our diffusion-based backbone exhibits
superior performance compared to other competitors, prov-
ing its exceptional perception capability for occluded hu-
man mesh recovery.
Furthermore, as illustrated in Fig-
ure 6, our diffusion-based backbone accurately captures the
occlusion-aware information in the cross-attention maps,
which provides explicit guidance for the subsequent regres-
1107
Table 3. Quantitative comparison on CMU-Panoptic [20]. We
measure the MPJPE across different subsets and calculate the
overall mean result. DPMesh obtains precise estimation for multi-
person scenarios, showing its adaptability to various occlusions.
Method
Haggl.
Maﬁa
Ultim.
Pizza
Mean
Zanﬁr et al. [52]
140.0
165.9
150.7
156.0
153.4
Zanﬁr et al. [53]
141.4
152.3
145.0
162.5
150.3
Jiang et al. [19]
129.6
133.5
153.0
156.7
143.2
ROMP [47]
111.8
129.0
148.5
149.1
134.6
REMIPS [11]
121.6
137.1
146.4
148.0
138.3
3DCrowdNet [7]
109.6
135.9
129.8
135.6
127.6
JOTR [28]
99.9
113.5
115.7
123.6
114.7
DPMesh (Ours)
97.2
109.8
114.3
120.5
110.4
Table 4. Ablation studies. We conduct ablations on 3DPW [50]
and 3DPW-OC [50, 57] to validate the effectiveness of the
diffusion-based backbone, the designed conditions and noisy key-
point reasoning. Notably, unlike other backbones that are pre-
trained on perception tasks, we ﬁnd that DPMesh yields com-
mendable results utilizing a generative pre-trained denoising U-
Net. Furthermore, our designed condition injection and noisy key-
point reasoning also enhance overall accuracy.
Settings
3DPW
3DPW-OC
MPJPE↓
PA-MPJPE↓
MPJPE↓
PA-MPJPE↓
type of conditioning inputs
z0
87.6
54.9
89.6
60.7
z0 + cj
75.1
49.8
73.7
50.6
z0 + cadd
j
100.3
62.0
109.0
73.4
z0 + cj + ct
73.6
47.4
70.9
48.0
noisy key-point reasoning
ResNet50 [15]
w/o NKR
80.2
52.4
78.9
52.8
HRNet-W48 [46] w/o NKR
78.7
50.9
76.9
51.8
ViT-L-16 [9]
w/o NKR
76.9
49.3
75.6
52.0
Swin-V2-L [32]
w/o NKR
77.0
48.8
76.1
52.2
DPMesh
w/o NKR
74.9
48.2
73.9
49.9
type of backbones
ResNet50 [15]
79.4
51.8
76.1
50.9
HRNet-W48 [46]
77.2
50.8
75.6
50.1
ViT-L-16 [9]
75.2
48.5
73.1
49.6
Swin-V2-L [32]
77.3
48.5
73.5
49.8
DPMesh (Ours)
73.6
47.4
70.9
48.0
sor by recognizing the target from various occlusions.
Designs of Conditions. We investigate the effects of dif-
ferent designs for conditioning inputs in our model. The
results are presented at the top of Table 4. We ﬁnd that both
the spatial heatmap condition cj and the joint coordinates
prompt ct can improve performance. This demonstrates the
effectiveness of these conditions in helping our model fo-
cus on critical areas. Moreover, we experiment with an-
other way to incorporate cj by element-wise adding it to the
image z0. However, this approach (z0 + cadd
j
) yields ter-
rible results that are even worse than the baseline without
cj condition. We assume that simply adding a highly ab-
stracted latent code with the heatmap is meaningless in the
latent space and introduces a misleading hint for the model
to learn. Consequently, we concatenate z0 and heatmap on
the channel dimension to reduce mutual interference.
Noisy Key-point Reasoning. We further investigate the ef-
fectiveness of our designed NKR approach. As shown in the
Cross-Attention Maps
Input Image
Predicted Meshes
Figure 6. Visualization of the cross-attention map. We demon-
strate that DPMesh successfully extracts occlusion-aware knowl-
edge. In the given examples, the male subject is partially occluded
by the female subject. The results demonstrate that our DPMesh
can precisely capture the spatial relationship and distinguish both
targets in cross-attention maps, even when one target is occluded
by the other. This enables proﬁcient recovery of occluded meshes.
middle part of Table 4, we disable NKR on various back-
bones and evaluate their performance. The results indicate
that all backbones without NKR perform worse in occlu-
sion scenes. For the diffusion backbone in DPMesh, NKR
approach provides a slight improvement, which conﬁrms its
capability to reduce the disturbance of noisy 2D observa-
tion. This demonstrates that NKR results in a more robust
framework, enabling DPMesh to handle challenging occlu-
sion scenarios and produce more accurate reconstructions.
5. Conclusion
In this paper, we introduce DPMesh, a simple yet effec-
tive framework for occluded human mesh recovery, which
fully exploits the rich knowledge about object structure and
spatial interaction within the pre-trained diffusion model.
We successfully tame the diffusion model with the designed
condition injection to perform accurate occluded mesh re-
covery in a single step. Furthermore, we leverage a noisy
key-point reasoning approach to enhance the robustness of
our model. Extensive experiments demonstrate our frame-
work can achieve accurate estimation even in severe occlu-
sion and crowded environments. We hope our work will
provide a new perspective for occluded human mesh recov-
ery and inspire more research in employing diffusion mod-
els for perception tasks.
Acknowledgement. This work was supported in part by the
National Natural Science Foundation of China under Grant
62125603, Grant 62321005, and Grant 62336004, and in
part by CCF-Tencent Rhino-Bird Open Research Fund.
1108
References
[1] Dragomir Anguelov, Praveen Srinivasan, Daphne Koller, Se-
bastian Thrun, Jim Rodgers, and James Davis. Scape: shape
completion and animation of people. In ACM SIGGRAPH,
pages 408–416. 2005. 3
[2] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh.
Realtime multi-person 2d pose estimation using part afﬁnity
ﬁelds. In CVPR, pages 7291–7299, 2017. 4
[3] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-
offrey Hinton. A simple framework for contrastive learn-
ing of visual representations. In ICML, pages 1597–1607.
PMLR, 2020. 5
[4] Hanbyel Cho and Junmo Kim. Generative approach for prob-
abilistic human mesh recovery using diffusion models. In
ICCVW, pages 4183–4188, 2023. 2, 3
[5] Hongsuk Choi, Gyeongsik Moon, and Kyoung Mu Lee.
Pose2mesh: Graph convolutional network for 3d human pose
and mesh recovery from a 2d human pose. In ECCV, pages
769–787. Springer, 2020. 1, 3, 6
[6] Hongsuk Choi, Gyeongsik Moon, JoonKyu Park, and Ky-
oung Mu Lee. Learning to estimate robust 3d human mesh
from in-the-wild crowded scenes. In CVPR, pages 1475–
1484, 2022. 3
[7] Hongsuk Choi, Gyeongsik Moon, JoonKyu Park, and Ky-
oung Mu Lee. Learning to estimate robust 3d human mesh
from in-the-wild crowded scenes. In CVPR, pages 1475–
1484, 2022. 1, 2, 5, 6, 7, 8
[8] Vasileios Choutas, Georgios Pavlakos, Timo Bolkart, Dim-
itrios Tzionas, and Michael J Black. Monocular expressive
body regression through body-driven attention. In ECCV,
pages 20–40. Springer, 2020. 1, 3
[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. In ICLR, 2020. 7,
8
[10] Runyang Feng, Yixing Gao, Tze Ho Elden Tse, Xueqing Ma,
and Hyung Jin Chang. Diffpose: Spatiotemporal diffusion
model for video-based human pose estimation.
In ICCV,
pages 14861–14872, 2023. 2, 3
[11] Mihai Fieraru, Mihai Zanﬁr, Teodor Szente, Eduard Baza-
van, Vlad Olaru, and Cristian Sminchisescu. Remips: Phys-
ically consistent 3d reconstruction of multiple interacting
people under weak supervision. NeurIPS, 34:19385–19397,
2021. 6, 8
[12] Lin Geng Foo, Jia Gong, Hossein Rahmani, and Jun Liu.
Distribution-aligned diffusion for human mesh recovery. In
ICCV, pages 9221–9232, 2023. 2, 3
[13] Riza Alp Guler and Iasonas Kokkinos. Holopose: Holistic 3d
human reconstruction in-the-wild. In CVPR, pages 10884–
10894, 2019. 3
[14] Rıza Alp G¨uler, Natalia Neverova, and Iasonas Kokkinos.
Densepose: Dense human pose estimation in the wild. In
CVPR, pages 7297–7306, 2018. 3
[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition.
In CVPR,
pages 770–778, 2016. 7, 8
[16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. NeurIPS, 33:6840–6851, 2020. 2,
3
[17] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-
Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
LoRA: Low-rank adaptation of large language models. In
ICLR, 2022. 6
[18] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian
Sminchisescu. Human3. 6m: Large scale datasets and pre-
dictive methods for 3d human sensing in natural environ-
ments. TPAMI, 36(7):1325–1339, 2013. 6
[19] Wen Jiang, Nikos Kolotouros, Georgios Pavlakos, Xiaowei
Zhou, and Kostas Daniilidis.
Coherent reconstruction of
multiple humans from a single image. In CVPR, pages 5579–
5588, 2020. 7, 8
[20] Hanbyul Joo, Hao Liu, Lei Tan, Lin Gui, Bart Nabbe,
Iain Matthews, Takeo Kanade, Shohei Nobuhara, and Yaser
Sheikh. Panoptic studio: A massively multiview system for
social motion capture. In ICCV, 2015. 2, 6, 7, 8
[21] Angjoo Kanazawa, Michael J Black, David W Jacobs, and
Jitendra Malik. End-to-end recovery of human shape and
pose. In CVPR, pages 7122–7131, 2018. 1, 2, 3, 6, 7
[22] Rawal Khirodkar, Shashank Tripathi, and Kris Kitani. Oc-
cluded human mesh recovery. In CVPR, pages 1715–1725,
2022. 3, 6, 7
[23] Muhammed Kocabas, Nikos Athanasiou, and Michael J
Black.
Vibe: Video inference for human body pose and
shape estimation. In CVPR, pages 5253–5263, 2020. 1, 3
[24] Muhammed Kocabas, Chun-Hao P Huang, Otmar Hilliges,
and Michael J Black. Pare: Part attention regressor for 3d hu-
man body estimation. In ICCV, pages 11127–11137, 2021.
1, 3, 6, 7
[25] Nikos Kolotouros, Georgios Pavlakos, Michael J Black, and
Kostas Daniilidis. Learning to reconstruct 3d human pose
and shape via model-ﬁtting in the loop.
In CVPR, pages
2252–2261, 2019. 1, 2, 3, 6, 7
[26] Nikos Kolotouros, Georgios Pavlakos, and Kostas Dani-
ilidis. Convolutional mesh regression for single-image hu-
man shape reconstruction.
In CVPR, pages 4501–4510,
2019. 7
[27] Jiefeng Li, Can Wang, Hao Zhu, Yihuan Mao, Hao-Shu
Fang, and Cewu Lu. Crowdpose: Efﬁcient crowded scenes
pose estimation and a new benchmark.
In CVPR, pages
10863–10872, 2019. 6
[28] Jiahao Li, Zongxin Yang, Xiaohan Wang, Jianxin Ma, Chang
Zhou, and Yi Yang. Jotr: 3d joint contrastive learning with
transformers for occluded human mesh recovery. In ICCV,
pages 9110–9121, 2023. 1, 3, 5, 6, 7, 8
[29] Zhihao Li, Jianzhuang Liu, Zhensong Zhang, Songcen Xu,
and Youliang Yan. Cliff: Carrying location information in
full frames into human pose and shape estimation. In ECCV,
pages 590–606. Springer, 2022. 6
[30] Kevin Lin, Lijuan Wang, and Zicheng Liu. End-to-end hu-
man pose and mesh reconstruction with transformers.
In
CVPR, pages 1954–1963, 2021. 1, 3
1109
[31] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
Pietro Perona, Deva Ramanan, Piotr Doll´ar, and C Lawrence
Zitnick. Microsoft coco: Common objects in context. In
ECCV, pages 740–755. Springer, 2014. 6
[32] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie,
Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al.
Swin transformer v2: Scaling up capacity and resolution. In
CVPR, pages 12009–12019, 2022. 7, 8
[33] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard
Pons-Moll, and Michael J Black. Smpl: A skinned multi-
person linear model. ACM ToG, 34(6), 2015. 3
[34] Xiaoxuan Ma, Jiajun Su, Chunyu Wang, Wentao Zhu, and
Yizhou Wang. 3d human mesh estimation from virtual mark-
ers. In CVPR, pages 534–543, 2023. 3
[35] Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Ger-
ard Pons-Moll, and Michael J. Black. Amass: Archive of
motion capture as surface shapes. In ICCV, 2019. 5
[36] Dushyant
Mehta,
Oleksandr
Sotnychenko,
Franziska
Mueller, Weipeng Xu, Srinath Sridhar, Gerard Pons-Moll,
and Christian Theobalt. Single-shot multi-person 3d pose
estimation from monocular rgb.
In 3DV, pages 120–130.
IEEE, 2018. 6
[37] Gyeongsik Moon, Ju Yong Chang, and Kyoung Mu Lee.
Poseﬁx: Model-agnostic general human pose reﬁnement net-
work. In CVPR, pages 7773–7781, 2019. 6
[38] Gyeongsik Moon, Hongsuk Choi, and Kyoung Mu Lee.
Neuralannot: Neural annotator for 3d human mesh training
sets. In CVPR, pages 2299–2307, 2022. 6
[39] Georgios Pavlakos, Luyang Zhu, Xiaowei Zhou, and Kostas
Daniilidis. Learning to estimate 3d human pose and shape
from a single color image. In CVPR, pages 459–468, 2018.
1, 3
[40] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani,
Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and
Michael J. Black. Expressive body capture: 3d hands, face,
and body from a single image. In CVPR, 2019. 3
[41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In ICML, pages 8748–8763. PMLR, 2021. 3
[42] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj¨orn Ommer. High-resolution image syn-
thesis with latent diffusion models. In CVPR, pages 10684–
10695, 2022. 2, 3, 5
[43] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:
Convolutional networks for biomedical image segmentation.
In MICCAI, pages 234–241. Springer, 2015. 3
[44] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
Cade Gordon,
Ross Wightman,
Mehdi Cherti,
Theo
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
man, et al. Laion-5b: An open large-scale dataset for train-
ing next generation image-text models. NeurIPS, 35:25278–
25294, 2022. 3
[45] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
and Surya Ganguli.
Deep unsupervised learning using
nonequilibrium thermodynamics.
In ICML, pages 2256–
2265. PMLR, 2015. 3
[46] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep
high-resolution representation learning for human pose esti-
mation. In CVPR, pages 5693–5703, 2019. 7, 8
[47] Yu Sun, Qian Bao, Wu Liu, Yili Fu, Michael J Black, and
Tao Mei. Monocular, one-stage, regression of multiple 3d
people. In ICCV, pages 11179–11188, 2021. 1, 2, 6, 7, 8
[48] Yu Sun, Qian Bao, Wu Liu, Yili Fu, Michael J Black, and
Tao Mei. Monocular, one-stage, regression of multiple 3d
people. In ICCV, pages 11179–11188, 2021. 3
[49] Yu Sun, Wu Liu, Qian Bao, Yili Fu, Tao Mei, and Michael J
Black. Putting people in their place: Monocular regression
of 3d people in depth. In CVPR, pages 13243–13252, 2022.
1, 3
[50] Timo von Marcard, Roberto Henschel, Michael Black, Bodo
Rosenhahn, and Gerard Pons-Moll. Recovering accurate 3d
human pose in the wild using imus and a moving camera. In
ECCV, 2018. 2, 6, 7, 8
[51] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan
Li, Hang Su, and Jun Zhu. Proliﬁcdreamer: High-ﬁdelity and
diverse text-to-3d generation with variational score distilla-
tion. NeurIPS, 36, 2024. 2, 3
[52] Andrei Zanﬁr, Elisabeta Marinoiu, and Cristian Sminchis-
escu. Monocular 3d pose and shape estimation of multiple
people in natural scenes-the importance of multiple scene
constraints. In CVPR, pages 2148–2157, 2018. 6, 8
[53] Andrei Zanﬁr, Elisabeta Marinoiu, Mihai Zanﬁr, Alin-Ionut
Popa, and Cristian Sminchisescu. Deep network for the in-
tegrated 3d sensing of multiple people in natural images.
NeurIPS, 31, 2018. 6, 8
[54] Andrei Zanﬁr,
Eduard Gabriel Bazavan,
Hongyi Xu,
William T Freeman, Rahul Sukthankar, and Cristian Smin-
chisescu. Weakly supervised 3d human pose and shape re-
construction with normalizing ﬂows. In ECCV, pages 465–
481. Springer, 2020. 3
[55] Hongwen Zhang, Yating Tian, Xinchi Zhou, Wanli Ouyang,
Yebin Liu, Limin Wang, and Zhenan Sun. Pymaf: 3d human
pose and shape regression with pyramidal mesh alignment
feedback loop. In ICCV, pages 11446–11456, 2021. 1, 3, 6,
7
[56] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding
conditional control to text-to-image diffusion models.
In
ICCV, pages 3836–3847, 2023. 2, 3, 5
[57] Tianshu Zhang, Buzhen Huang, and Yangang Wang. Object-
occluded human shape and pose estimation from a single
color image. In CVPR, 2020. 1, 2, 3, 6, 8
[58] Wenliang Zhao, Yongming Rao, Zuyan Liu, Benlin Liu, Jie
Zhou, and Jiwen Lu.
Unleashing text-to-image diffusion
models for visual perception. ICCV, 2023. 2, 3, 5
1110
